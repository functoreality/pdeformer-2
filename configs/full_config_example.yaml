# An example configuration file containing all possible options.
# This file is provided for the convenience to the users, and will not be used.
model_type: pdeformer  # {pdeformer, deeponet, cnn_deeponet, unet2d, fno2d, fno3d}
model:
  graphormer:
    num_node_type: 128
    num_in_degree: 32
    num_out_degree: 32
    num_spatial: 16
    num_encoder_layers: 12
    embed_dim: 768
    ffn_embed_dim: 1536
    num_heads: 32
    pre_layernorm: True
  scalar_encoder:
    dim_hidden: 256
    num_layers: 3
  function_encoder:
    # Available options for 'type': {deepset, weighted_deepset, pb_wdpset,
    # patched1d, patched2d, patched2dconv, patchset2d, conv2d, cnn2dv2}
    type: cnn2dv3
    num_branches: 4
    deepset_point_fn: poly_inr  # {mlp, poly_inr, siren, mfn}; list allowed for pb_wdpset
    dim_hidden: 512
    num_layers: 3
    resolution: 128  # for 'type' in {patched, conv2d}
    conv2d_input_txyz: False
    cnn_keep_nchw: True  # False is logically correct, but makes training unstable
  multi_inr:
    enable: False
    separate_latent: False
  inr:
    type: poly_inr  # {siren, mfn, poly_inr}
    num_layers: 12
    dim_hidden: 768
    siren:
      enable_scale: True
      num_pos_enc: 0  # 0 means no position encoding
    mfn:
      filter_type: dino_fourier  # {original_fourier, dino_fourier, gabor}
      enable_scale: False
      input_scale: 256.0  # 256 for dino_fourier, 64 for original_fourier, ? for gabor
      gabor_alpha: 6.0
      gabor_beta: 1.0
    poly_inr:
      enable_affine: False
      enable_shift: True
      enable_scale: True
      modify_he_init: False
      affine_act_fn: identity  # {identity, lrelu, sin}
      activation_fn: sin  # {lrelu, sin}
  inr2:
    type: mfn  # {siren, mfn, poly_inr}
    num_layers: 12
    dim_hidden: 768
    siren:
      enable_scale: True
      num_pos_enc: 0  # 0 means no position encoding
    mfn:
      filter_type: gabor  # {original_fourier, dino_fourier, gabor}
      enable_scale: True
      input_scale: 256.0  # 256 for dino_fourier, 64 for original_fourier, ? for gabor
      gabor_alpha: 6.0
      gabor_beta: 1.0
    poly_inr:
      enable_affine: False
      enable_shift: True
      enable_scale: True
      modify_he_init: False
      affine_act_fn: identity  # {identity, lrelu, sin}
      activation_fn: sin  # {lrelu, sin}
  hypernet:
    dim_hidden: 512
    num_layers: 2
    shared: False  # whether the parameters of all INR layers are generated by the same hypernet
  lora:
    mode: disabled  # {disabled, create, load, merge}
    exclude_layers: None  # We assume no module name starts with 'None'.
    target_modules: "."  # Default applied to all dense (linear) cells.
    lora_rank: 8
    lora_alpha: 16
  load_ckpt: none  # or path/to/your/model-size.ckpt
data:
  path: ../data_download  # or any path/to/your/data_download
  type: multi_pde  # {single_pde, multi_pde}
  num_workers: 8
  num_samples_per_file:
    train: 1000
    regularize: 1000  # pdeformer+single_pde only
    test: 1000
  pde_dag:
    max_n_scalar_nodes: 192
    max_n_function_nodes: 16
    disconn_attn_bias: -inf
  single_pde:
    param_name: pdegym_wave  # {sinegordon_fname, ins_coupled_fname, ins_fname, pdegym_wave, rigno_wave}
    regularize_ratio: 0.0  # for pdeformer, introduce multi_pde data for regularization
    train: [gauss-norm]
    test: [gauss-norm]  # same as train if this entry is not set
  multi_pde:
    train:
      dcr_base: dedalus_v5.1_DiffConvecReac2D_hom_cU1_k1e-03_0.01_seed2
      dcrLgK_disk:
        # Here 'LgK' means the diffusive coefficient (kappa) is represented in
        # the (base 10) exponential form, and the scalar/function encoder
        # receives lg(kappa) as its input.
        - dedalus_v5.1_DiffConvecReac2D_hom_disk_cU1_k1e-03_0.01_seed1
        - dedalus_v5.1_DiffConvecReac2D_hom_disk_cU1_k1e-03_0.01_seed2
      wave_npX:
        format: dedalus_v5.1_Wave2D_hom_npX_sJ3_cU1_k1e-02_4_seed%d
        begin: 3
        num: 90
        step: 2
      mvdcr_2:
        format: dedalus_v5.1_MCompn2D_hom_nv2_cU1_k1e-03_0.01_seed%d
        begin: 2
        num: 160
    test:
      dcr_base: dedalus_v5.1_DiffConvecReac2D_hom_cU1_k1e-03_0.01_seed1
      dcr_disk: dedalus_v5.1_DiffConvecReac2D_hom_disk_cU1_k1e-03_0.01_seed0
      wave_npX: dedalus_v5.1_Wave2D_hom_npX_sJ3_cU1_k1e-02_4_seed1
      mvdcr_2: dedalus_v5.1_MCompn2D_hom_nv2_cU1_k1e-03_0.01_seed1
  dynamic:  # training with dynamic buffer dataset
    enabled: False
    wait_time: 4  # time interval (s) between two consequent queries
    log_detail: True
    type_default:
      n_logical_dataset: 10
      max_inactive_files: 5  # need <= n_logical_dataset
    type_custom:  # Remove this entry if no custom values need to be set.
      - types: [mvdcr_2]
        n_logical_dataset: 4
        max_inactive_files: 2
      - types: [dcr_base, dcrLgK_disk]
        n_logical_dataset: 1
        max_inactive_files: 1
    obsutil: /home/ma-user/work/obsutil_linux_arm64_5.5.12/obsutil
    obs_path:
      dcr: obs://data-download/dedalus_v5.1/DiffConvecReac2D
      wave: obs://data-download/dedalus_v5.1/Wave2D
      mvdcr: obs://data-download/dedalus_v5.1/MCompn2D
      mvdcrLgK: obs://data-download/dedalus_v5.1/MCompn2D
      mvwave: obs://data-download/dedalus_v5.1/MCWave2D
      dcdcr: obs://data-download/dedalus_v5.1/DivConstrDCR2D
      dcwave: obs://data-download/dedalus_v5.1/DivConstrWave2D
      swe: obs://data-download/dedalus_v5.1/SWE2D
      elasticsteady: obs://data-download/fenicsx_v2.1/ElasticSteady2D
      dag_info: obs://data-download/dag_info_v5
    remove_type: file  # {none, file, file+dag}
    upload_dag: False
train:
  total_batch_size: 80  # we use 10 per NPU device in our experiments
  num_txyz_samp_pts: 8192
  lr_init: 1.e-4
  epochs: 10
  loss:
    type: RMSE  # {MSE, RMSE, MAE, MIXED, SPECTRAL}
    sample_reduce: mean  # {mean, nanmean, sum, nansum}
    normalize: True
    normalize_eps: 0.05
    mixed_weight: 0.1
    spectral_modes: 12
    spectral_sqrt: True
  optimizer: Adam  # {Adam, AdamW}
  weight_decay: 0.0
  lr_scheduler:
    type: cos  # {mstep, cos, exp}
    milestones: [0.6, 0.8, 1]  # mstep only
    decay: 0.5  # mstep/exp only
    enable_warmup: True
    warmup_epochs: 10
  grad_clip_value: 1  # -1 means no gradient clipping
  # About 'prefix' fine-tune mode: 1. May need to remove the first 'eval_model' in 'train.py';
  # 2. Names of the model parameters can be checked using the code:
  # print([param.name for param in model.trainable_params()])
  module_list: [all]  # {all, lora, inr, graphormer, function_encoder, prefix=PREFIX}
  dw_penalty:  # penalize change of model weights during fine-tuning
    # Note: This functionality is disabled in code. To re-enable it, users need
    # to uncomment the two lines containing 'dw_penalty' in train.py.
    mode: disabled  # {disabled, fix_coef, fix_distance (todo)}
    init_coef: 0.1
    distance_fn: MSE  # {MSE, RMSE, MAE}
    distance_val: 1  # 'fix_distance' only; not implemented yet
eval:
  total_batch_size: 24  # we use 3 per NPU device in our experiments
  interval: 25
  plot_num_per_type: 1
  dataset_per_type: 1  # when >=0, only use this prescribed number of datasets in the eval loop for each PDE type. 0 means skipping eval loop.
inverse:
  data_source: multi_pde
  pde_type: dcr  # currently only supports {dcr, wave}
  data_file: dedalus_v5.1_inv_DiffConvecReac2D_hom_cU1_k1e-03_0.01_seed1
  pde_cases: 4  # int or list of ints
  num_samples_per_pde: 10
  system_identification: False
  observation:
    ic_noise:
      type: uniform  # {none, uniform, normal}
      level: 0.01
    noise:
      type: uniform  # {none, uniform, normal}
      level: 0.01
    xyz_location:
      type: random  # {all, equispaced, last, random}
      num_pts: 128
    t_location:
      type: all_random  # {all, equispaced, last, t_random, all_random}
      num_pts: 20
  plot_num_per_cls: 2
  loss:
    type: RMSE  # {MSE, RMSE, MAE, MIXED}
    normalize: True
    normalize_eps: 0.05
    mixed:
      weight: 0.1
  coef:
    coef_scale: 1.0
    enable_nu: False  # ignore the first scalar, whether nu or not
    num_coef: 10
    pso:
      pop_size: 50
      max_gen: 100
  func:
    function_node_id: 1  # 1 for s(x) of dcr (homogeneous case), 2 for a(x) of wave
    function_regularize:
      type: L2  # {L1, L2, squareL2}
      weight: 1.e-2
    epochs: 1000
    learning_rate: 0.1
    weight_decay: 0.0
    lr_scheduler:
      type: mstep
      milestones: [1.0]
      lr_decay: 0.8
    fwi: False
deeponet:  # applies to {deeponet, cnn_deeponet}
  trunk_dim_hidden: 256
  trunk_num_layers: 6
  branch_dim_hidden: 256
  branch_num_layers: 6
  dim_out: 2048
  num_pos_enc: 0  # 0 means no position encoding
fno3d:  # applies to {unet2d, fno2d, fno3d}
  modes: 12
  channels: 20
  depths: 4
record_dir: "exp/debug"
